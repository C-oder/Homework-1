{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"7d1a89d9-7c2c-4f0e-8a8d-c5c6a4e2c5b8\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# k-NN Classification Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates the complete workflow for implementing the k-Nearest Neighbors (k-NN) algorithm from scratch. The dataset consists of 178 instances with 13 numerical features and a class label (1, 2, or 3). We will:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Load and visualize the dataset\\n\",\n",
    "    \"- Preprocess the data (handle missing values, normalize, split into training/testing sets)\\n\",\n",
    "    \"- Implement k-NN from scratch (using Euclidean and Manhattan distance metrics)\\n\",\n",
    "    \"- Evaluate model performance for different K values\\n\",\n",
    "    \"- Plot accuracy vs. K\\n\",\n",
    "    \"- Display a confusion matrix and classification report for the best-performing model\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"4b4f3c2a-840b-4e31-a8b8-5c3583046c86\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Data Loading and Preprocessing\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this section, we load the dataset, handle missing values, normalize the data, and split it into training and testing sets.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"3a1442a6-7cbd-4e9f-90b9-97f5c48e6a10\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from sklearn.preprocessing import MinMaxScaler\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ----------------------------------------------\\n\",\n",
    "    \"# Load the Dataset\\n\",\n",
    "    \"# ----------------------------------------------\\n\",\n",
    "    \"# If you have a local CSV from the extracted zip, uncomment and adjust the following lines:\\n\",\n",
    "    \"# df = pd.read_csv('wine.csv')\\n\",\n",
    "    \"# X = df.iloc[:, :-1]   # all columns except the last one\\n\",\n",
    "    \"# y = df.iloc[:, -1]    # the last column (class labels)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Alternatively, use the ucimlrepo package to fetch the dataset\\n\",\n",
    "    \"from ucimlrepo import fetch_ucirepo\\n\",\n",
    "    \"wine = fetch_ucirepo(id=109)  \\n\",\n",
    "    \"X = wine.data.features.copy()  # making an explicit copy to avoid warnings\\n\",\n",
    "    \"y = wine.data.targets\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ----------------------------------------------\\n\",\n",
    "    \"# Preprocess the Data\\n\",\n",
    "    \"# ----------------------------------------------\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Handle missing values (if any)\\n\",\n",
    "    \"X.fillna(X.mean(), inplace=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Normalize features (scale values between 0 and 1)\\n\",\n",
    "    \"scaler = MinMaxScaler()\\n\",\n",
    "    \"X_scaled = scaler.fit_transform(X)\\n\",\n",
    "    \"X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Split data into Training (80%) and Testing (20%) sets\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Training set size:\\\", X_train.shape)\\n\",\n",
    "    \"print(\\\"Testing set size:\\\", X_test.shape)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"a387d2df-d973-4f97-8d7d-45b9b0c1a5e2\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Data Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's visualize some of the features to see if they overlap across different classes. Here we create a scatter plot of **Alcohol** vs. **Malicacid**, colored by the class label.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a3b1f9ab-3e2a-4af1-8b27-cc5dbb5d9d5b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Scatter plot: Alcohol vs. Malicacid\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"sns.scatterplot(x=X['Alcohol'], y=X['Malicacid'], hue=y, palette=\\\"viridis\\\")\\n\",\n",
    "    \"plt.title(\\\"Alcohol vs. Malicacid by Class\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Alcohol\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Malicacid\\\")\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"aa1b1374-d4ea-4324-8ad3-7e76e3c6f4f2\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. k-NN Implementation from Scratch\\n\",\n",
    "    \"\\n\",\n",
    "    \"We implement the k-NN algorithm without using `sklearn.neighbors.KNeighborsClassifier`. Two distance metrics are used: **Euclidean** and **Manhattan**.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"d0f91f60-54b8-4cf2-bd25-9a5918c8a8fa\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from collections import Counter\\n\",\n",
    "    \"\\n\",\n",
    "    \"def euclidean_distance(p1, p2):\\n\",\n",
    "    \"    return np.sqrt(np.sum((p1 - p2) ** 2))\\n\",\n",
    "    \"\\n\",\n",
    "    \"def manhattan_distance(p1, p2):\\n\",\n",
    "    \"    return np.sum(np.abs(p1 - p2))\\n\",\n",
    "    \"\\n\",\n",
    "    \"def knn_predict(X_train, y_train, X_test, k, distance_metric=\\\"euclidean\\\"):\\n\",\n",
    "    \"    predictions = []\\n\",\n",
    "    \"    X_train_np = X_train.to_numpy()\\n\",\n",
    "    \"    X_test_np = X_test.to_numpy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for test_point in X_test_np:\\n\",\n",
    "    \"        distances = []\\n\",\n",
    "    \"        for idx, train_point in enumerate(X_train_np):\\n\",\n",
    "    \"            if distance_metric == \\\"euclidean\\\":\\n\",\n",
    "    \"                dist = euclidean_distance(test_point, train_point)\\n\",\n",
    "    \"            elif distance_metric == \\\"manhattan\\\":\\n\",\n",
    "    \"                dist = manhattan_distance(test_point, train_point)\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                raise ValueError(\\\"Unsupported distance metric\\\")\\n\",\n",
    "    \"            distances.append((dist, y_train.iloc[idx]))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Sort distances and select k nearest neighbors\\n\",\n",
    "    \"        distances.sort(key=lambda x: x[0])\\n\",\n",
    "    \"        k_nearest = [label for _, label in distances[:k]]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Majority vote\\n\",\n",
    "    \"        most_common = Counter(k_nearest).most_common(1)[0][0]\\n\",\n",
    "    \"        predictions.append(most_common)\\n\",\n",
    "    \"    return np.array(predictions)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"29b4c671-2409-44a5-8c0b-b640f1e9d9e7\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Model Evaluation and Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"We evaluate the k-NN model for different values of **K** (1, 3, 5, 7, 9) using both distance metrics. We calculate the classification accuracy for each K, plot accuracy vs. K, and generate a confusion matrix and classification report for the best-performing configuration.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"d6efae60-8a53-4b8e-9367-10a993c3f4e8\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\n\",\n",
    "    \"\\n\",\n",
    "    \"k_values = [1, 3, 5, 7, 9]\\n\",\n",
    "    \"euclidean_accuracies = []\\n\",\n",
    "    \"manhattan_accuracies = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for k in k_values:\\n\",\n",
    "    \"    # Euclidean distance evaluation\\n\",\n",
    "    \"    y_pred_euclidean = knn_predict(X_train, y_train, X_test, k, distance_metric=\\\"euclidean\\\")\\n\",\n",
    "    \"    acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\\n\",\n",
    "    \"    euclidean_accuracies.append(acc_euclidean)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Manhattan distance evaluation\\n\",\n",
    "    \"    y_pred_manhattan = knn_predict(X_train, y_train, X_test, k, distance_metric=\\\"manhattan\\\")\\n\",\n",
    "    \"    acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\\n\",\n",
    "    \"    manhattan_accuracies.append(acc_manhattan)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"K = {k}: Euclidean Accuracy = {acc_euclidean:.4f}, Manhattan Accuracy = {acc_manhattan:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"13f1e7c8-c98b-4d8a-8c6b-70193bfe4d9b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot Accuracy vs. K\\n\",\n",
    "    \"plt.figure(figsize=(8, 5))\\n\",\n",
    "    \"plt.plot(k_values, euclidean_accuracies, marker='o', label='Euclidean')\\n\",\n",
    "    \"plt.plot(k_values, manhattan_accuracies, marker='s', label='Manhattan')\\n\",\n",
    "    \"plt.xlabel(\\\"K Value\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Accuracy\\\")\\n\",\n",
    "    \"plt.title(\\\"Accuracy vs. K for k-NN\\\")\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a04b8e59-7e16-4a2e-b956-cf74d25a91d5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Choose the best K based on Euclidean accuracy\\n\",\n",
    "    \"best_index = np.argmax(euclidean_accuracies)\\n\",\n",
    "    \"best_k = k_values[best_index]\\n\",\n",
    "    \"print(f\\\"\\\\nBest K based on Euclidean distance: {best_k}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get predictions for best K\\n\",\n",
    "    \"best_predictions = knn_predict(X_train, y_train, X_test, best_k, distance_metric=\\\"euclidean\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confusion Matrix and Classification Report\\n\",\n",
    "    \"cm = confusion_matrix(y_test, best_predictions)\\n\",\n",
    "    \"cr = classification_report(y_test, best_predictions)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nConfusion Matrix:\\\")\\n\",\n",
    "    \"print(cm)\\n\",\n",
    "    \"print(\\\"\\\\nClassification Report:\\\")\\n\",\n",
    "    \"print(cr)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"2ac1195a-73c8-4eeb-9556-b3225d3a8146\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Conclusion\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this notebook, we loaded and preprocessed the dataset, visualized the distribution of key features, and implemented a k-NN classifier from scratch using both Euclidean and Manhattan distance metrics. We evaluated the model for various values of K, analyzed the impact on accuracy, and presented a confusion matrix and classification report for the best-performing configuration. This process provides insights into how different parameters affect k-NN performance and offers a complete workflow for implementing and analyzing a machine learning algorithm.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.x\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
